---
title: "Gaussian Kernel Density Estimation with a Data-Derived Mixture Prior: a White
  Paper"
author: "Cory Lanker"
date: "`r Sys.Date()`"
---

# Introduction
\label{sect:intro}
Suppose you want a kernel density estimate (KDE) of
a population in $\mathbb{R}^p$ based
on $N$ iid observations from this population.
Suppose further that it is natural to assume
$K$ subpopulations comprise this population space.
One way to get such a KDE for such a population
is with a Gaussian mixture model that estimates 
these individual subpopulations, 
but to fit this model requires specification 
of $M$, the number of components.
Mixture models are natural to consider 
when a population is composed of two or more subpopulations,
and these types of population distributions are common in
many scientific disciplines~\citep{kang2009clusterwise}.
We envision a complex population space with large $N$
and potentially large $p$ and $M$ that makes
an adequate guess of the number of subpopulations difficult.
With unknown $M$,
the EM algorithm requires estimation of the number of subpopulations;  
such estimation is a difficult problem 
as information criteria are unreliable~\citep{mclachlan2004finite}.

With Bayesian nonparametrics we instead form
a Gaussian mixture model to estimate
the density via a truncated Dirichlet process
as outlined in Chapter 22 in \cite{gelman2013bayesian}.
To get realizations from this prior
we use the stick-breaking construction 
of \cite{sethuraman1994constructive}.
A Gaussian mixture model based on the Dirichlet process 
can approximate many types of conditional densities while balancing
fit smoothness with an adequate amount of
local linearities~\citep{wade2014predictive}.
A nonparametric Bayes approach %is desirable because it 
with a Dirichlet process prior for the mixture proportions
does not require estimation of the number of densities.
Unfortunately, (p.535): 
"It is important to avoid choosing a [prior on model parameters]
that is improper, or even diffuse but proper, as the results
may be sensitive to the size of the enormous variance chosen.
Instead, best results are obtained when [the prior] is chosen to generate
mixture components that are close to the support of the data."
The two ways listed to accomplish this in that paragraph fall short.
First, normalizing the data won't do anything meaningful in high dimensions
when subpopulations are likely spread far apart 
from each other in $\mathbb{R}^p$.
Second, using hyperparameters based on one's knowledge will also
be ineffective in high dimensions for the same reason.

Suppose we some recursive-component clustering method
to find the best Gaussian mixture model for KDE 
using for a mixture of two components, 
then three components, etc., 
until some chosen final number of components.
Afterwards, we have a collection of 
$L$ Gaussian mean and covariance parameters
the various subpopulations could have,
but don't know which combination of collection elements form
the best Gaussian mixture model.
In this white paper, we propose a Bayesian nonparametric
method that yields the best such combination within this collection.
This method extends the hierarchical Bayes Gaussian mixture model
of \cite{gelman2013bayesian} and Bayesian curve fitting approach
of  \cite{muller1996bayesian}
by constructing a mixture prior for the mixture model's Gaussian parameters
using this collection of proposed subpopulation Gaussian parameters.
Since the provided collection of Gaussian parameters
are in the prior distribution, the posterior distribution mixture parameters
will deviate from the parameters of the provided collection
according to the likelihood function (i.e., according to the data).

Additionally, is we assume that a subset of the $p$ components
represent an output value or vector, we can use
the posterior model to estimate the regression function.
For such estimation, this model allows 
flexible prediction in a manner
mimicking locally linear regression functions.
In this paper, we consider 
estimation of a multivariate regression function
for data from a mixture of multivariate normal densities. 

A Bayesian mixture model for regression is outlined 
in _Bayesian Data Analysis, 3rd ed._ in Section 22.5. 
In that section, the authors propose
to predict univariate $y_{n+1}$ given observed $x_{n+1}$ based on $n$ items
of training data $w_i=(y_i, x_i) \in \mathbb{R}^{p+1}$, 
and then modeling the joint density of $w$
using $H$ multivariate normals:
$$f(w_i) = \sum_{h=1}^H \pi_h N_{p+1}(w_i|\mu_h,\Sigma_h).$$
A multivariate normal mixture model results in the conditional density for $y_{n+1}$ given $x_{n+1}$:
$$f(y_i|x_i)=\sum_{h=1}^H \hat\pi_h(x_i)N(y_i|\beta_{0,h}+\beta_{1,h}x_i, \sigma_h^2),$$
where $\hat\pi_h(x_i)$ are the estimates that $x_i$ is in component $h$:
$$\hat\pi_h(x_i) = \frac{\pi_h N_p(x_i|\mu_h^{(x)}, \Sigma_h^{(x)}}
{\sum_{h'=1}^H \pi_{h'} N_p(x_i|\mu_{h'}^{(x)}, \Sigma_{h'}^{(x)}}.$$

The authors state that estimation of the model parameters
$\{\pi_h, \mu_h, \Sigma_h: h=1,\dots,H\}$ is done with Gibbs sampling of
the posterior using the "approach outlined earlier" in the chapter.
On p.522, after defining $\pi_h$, the likelihood of a joint $w=(y,x)$ is defined over
its latent class structure, conditioning on the latent $z_i$ assigned to one of $\{1, \dots, H\}$:
$$g(w|\pi, \mu, \Sigma) = \sum_{h=1}^H\pi_h N(w|\mu_h,\Sigma_h),$$
which according to the authors defines a finite mixture with $H$ components with
weights $\pi$. The motivation of finite mixture models is to adequately approximate
any density. 

Ideas for future work:
* An idea for next step: let mixture components $H+1$ to be a "prior"
with a small weight, using mean $\bar{x}$ and covariance as a very diffuse estimate 
of the empirical variance.
* 2nd idea: perhaps initial crude estimates are required to efficiently
run the Gibbs sampler, or to have some data points in many of the clusters
-- but as p. 523 says, these initial estimates are usually not needed because
they are drawn in the Gibbs sampler, but I still feel that one could get into
an uncommon area of the posterior, see my example using a simple bivariate
normal posterior.
* Idea 3: p.524 says a variety of posterior predictive checks in Section 22.2 can 
be used to assess the fit of the model, and sensitivity of inferences
to "assumed parametric family" can be evaluated using methods of Chapter 17.
* Idea 4: dynamically adjust $H$ based on previous allocations.
* Idea 5: Would you consider only running the program on missing data?
Meaning any predictions would be X without Y, 
and you can have partially-missing X too.

The model in Section 22.3 is the following:

* Component proportions $\pi_h$: 
$(\pi_1, \dots, \pi_H) \sim \text{Dirichlet}(a, \dots, a)$.
* Gaussian parameters $\theta_h$: $\theta_h \sim P_0$
where conjugate $P_0$ is a common prior from which
mixture component-specific parameters are drawn.

For the Gaussian mixture model $\mu_h$ and $\Sigma_h$
are drawn from a conditionally conjugate distribution
that is Normal Inverse-Wishart, with four hyperparameters:

* $\mu$ prior mean $\mu_0$
* $\mu$ prior variance factor $\kappa$
* $\Sigma$ prior $\rho$, degrees of freedom
* $\Sigma$ prior mean $S_0$

In this way, we define a Gibbs sampler that alternates between (1) sampling
latent $z_i$ indexes, (2) updating component parameters $\mu_h$
and $\Sigma_h$ given the conjugate conditional posterior distribution considering
the data points assigned to component $h$ after the previous step,
and (3) updating the mixture probabilities $\pi$.

The posterior is analyzed using the paragraph after point 3 on p.534:
the density $g(w)$ is computed at points $w$ using the estimate
$\frac 1 S \sum_{h=1}^H \pi_h^{(s)} N(w|\mu_h^{(s)}, \Sigma_h^{(s)})$
where $s$ are the saved iterates.

We construct a model without mixture component identifiers 
for the   observations. 
Prediction   requires
estimation of the observation component membership
along with the mixture density parameters. 
We include indicators in our probability model 
to express these missing data.

# Hierarchical Bayes modeling with GMMs
\label{sect:p1a}

A typical hierarchical Bayes model
for a mixture of multivariate normal densities
uses a normal--inverse-Wishart prior.
This diffuse prior induces too much flatness 
in the posterior distribution %that itself becomes too diffuse
to give prediction results competitive with other smoothing methods.
The challenge of using a Bayesian mixture model for prediction
is focusing the prior on the data in a principled approach
that generates mixture components close to the 
support of the data \citep{gelman2013bayesian}.
The proposed prediction method uses the data to
form a mixture prior on the %component densities'
Gaussian parameters in the mixture model.
Our model has fully defined conditional distributions
for all parameters allowing efficient Gibbs sampling of
the posterior distribution.

A variety of test cases shows results when using
this mixture prior Bayesian mixture model for prediction.
First, the locally linear bivariate Doppler function
demonstrates that the mixture prior method 
adapts predicted values to the local
structure of the regression function.
Second, we assess performance using %present a comprehensive 
a simulation study with 8 and 16 predictors. %a regression function
Last, applying our mixture prior method on 
standard regression data sets gives %similar 
prediction results competitive with local regression methods.   

We have observations $z_i \in \mathbb{R}^p$ 
for $i=1, \dots, N_{obs}$
and wish to estimate the density of 
the population density of $p$-dimensional $z$.
Perhaps $z$ represents
$(x, y)$ with an aim to
predict the response $y^*$ for a new $x^*$
%Consider the problem of predicting $y$ given $x$
that minimizes squared error loss.
We use the mixture model described in Section~\ref{sect:intro}
without a priori specification of the number of mixture components
or of individual observations to mixture components.
Let parameter $\theta_i$ represent the multivariate 
Gaussian parameters %mean and covariance matrix
for observed $p$-dimensional $z_i$
yielding the likelihood function
$\prod_{i=1}^{N_{obs}} N_{p}(z_i|\theta_i)$,
where $N_{p}(z_i| \theta_i)$ represents the density of
a multivariate Gaussian distribution with mean and variance parameters
$\theta_i$ evaluated at $z_i$.

## Population probability model
Model the distribution of 
$z \in \mathbb{R}^p$ with a mixture of $H$ components,
with unknown value for $H$.
Define parameter $\lambda$ as the collection of
population proportions $\lambda_h$   
for components $h=1,\dots,H$,   
with $\sum_{h=1}^H \lambda_h=1$.
The sampling distribution of 
$z$
from this population has the density
$$p(z|\lambda, \xi) = \sum_{h=1}^H \lambda_h\, N_{p}(z|\xi_h),$$
where $\xi_h$ denotes %each component's 
the multivariate Gaussian   
mean and variance 
parameters for component $h$.
Such a model captures dissimilarity 
in the Gaussian parameters 
across the population mixture.  

## Dirichlet process model
We approximate
the population mixture distribution 
of (\ref{eqn:popmix})
with the following Dirichlet process model employing a
latent multivariate Gaussian mixture. 
Let $\pi_m$  and $\eta_m$ represent 
the proportion and
Gaussian parameters
for component $m=1,\dots,\infty$ of this latent
mixture,
where proportions $\pi_m$ sum to one. % $\sum_{m=1}^\infty \pi_m=1$.
Let parameter $\eta_m$ consist of the Gaussian mean $\mu_m$
and variance $\Sigma_m$ for component $m$. 
For practical purposes we truncate this
Dirichlet process mixture at $M$ components.
We select $M$ large enough such the distribution of
obtained from our model 
$\sum_{m=1}^M \pi_m\,N_{p}(z|\eta_m)$
closely approximates the population density
of (\ref{eqn:popmix}).

Given this Dirichlet model
and estimates for the component proportion and Gaussian density 
parameters $\pi$ and $\eta$,
prediction of $y^*$ for a new $x^* \in \mathbb{R}^d$,
in the $z \equiv (x, y)$ usage, 
follows from the sum of conditional Gaussian densities
(see BDA3 p.542, Monahan p.116)
$$\sum_{m=1}^M p_m(x^*) N_{(p-d)}(y^*|x^*, \mu_m, \Sigma_m).$$
The component membership proportion $p_m$ at $x^*$ represents
the probability that point~$x^*$ is from the Gaussian density of component $m$.
The formula is 
$$p_m(x^*) = h_m(x^*) / \big(\sum_{m'=1}^M h_{m'}(x^*)\big),$$
where $h_m(x^*) = \pi_m\, N_d(x^*|\mu_{m,x},\Sigma_{m,xx})$
and $\mu_{m,x}$ and $\Sigma_{m,xx}$ are the marginal parameters for $x$.

## Prior distribution
We follow a hierarchical Bayesian approach
by defining an appropriate prior
on model parameters $\eta$, $\pi$, and $\theta$, using 
the resulting posterior distribution for prediction.
The prior distribution for Gaussian distribution parameters $\eta$, 
denoted $g(\eta)$,
is an extension of the normal--inverse-Wishart prior for $(\mu, \Sigma)$
of \cite{muller1996bayesian}. 
The normal--inverse-Wishart prior has the setup
\citep{gelman2013bayesian} (using BDA2 p.87)
$$\Sigma \sim \text{Inv-Wishart}_{\rho}\!\left(\Lambda_0^{-1}\right)\\
\mu|\Sigma \sim \text{Normal}\left(\mu_0, \Sigma/\kappa_0\right)$$
which has a joint density proportional to
$$|\Sigma|^{-(\rho+p+1)/2}
\exp\!\left(-\frac12
\tr\left(\Lambda_0\Sigma^{-1}\right)\right)
\times|\Sigma|^{-1/2}
\exp\!\left(
-\frac{\kappa_0}2 (\mu - \mu_0)^T\Sigma^{-1}(\mu-\mu_0)\right).$$
Such prior is insufficient for discovering the Gaussian
mixtures of a population
when the true subpopulation densities $\phi(z|\mu_h,\Sigma_h)$
have very different covariance structures. 

In this version, I outline a prior that may 
utilize prior mass more efficiently.
The $(\mu_m, \Sigma_m)$ prior from
the previous version of this paper, though greatly culling
the ineffective prior mass from the single mean-covariance prior
offered in Chapter 22 in \cite{gelman2013bayesian},
may still suffer from having too much ineffective prior mass.
We extend this prior by composing a mixture distribution
for the component mean~$\mu$ prior
using multivariate Gaussian densities
and variance~$\Sigma$ prior 
using inverse-Wishart densities.

I propose the joint prior based on $L$ combinations of $(l_k, M_k)$,
where $l_k$ is a location and $M_k$ is a covariance matrix.
$$g_1(\mu_m, \Sigma_m) \propto 
\sum_{k=1}^L |\Gamma_k|^{-1/2}
w_k |\Sigma_m|^{-(\rho_k+p+2)/2} |M_k|^{\tfrac 1 2 \rho_k}
e^{-\tfrac 1 2 (\mu_m - l_k)' \Gamma_k^{-1} (\mu_m - l_k)
   -\tfrac 1 2 \text{tr}(M_k \Sigma_m^{-1})},$$
which is a product mixture of a multivariate Gaussian $\mu_m$ prior
and inverse-Wishart $\Sigma_m$ prior, for all $m = 1, \dots, n$ components of
the over-arching Gaussian mixture model.

Coupled with the unchanged priors for the $N_{obs}$ latent $\theta_i$
and $n-1$ mixing proportion parameters $\phi_m$:
$$g_2(\theta_i) = \mathbb{I}\{\theta_i \in \eta_1, \dots, \eta_n\} 
\;\text{and}\;
g_3(\phi_m) \sim \text{Beta}(\alpha, \beta).$$



# Appendix
## Derivation ofPosterior Conditional Distributions

Conditional distributions can be found from (\ref{eqn:post1}) 
in terms of the individual 
$\mu_m$, $\Sigma_m$, $\phi_m$, and $\theta_i$
to allow Gibbs sampling of the posterior~\citep{gelman2013bayesian}. 
The exact posterior distribution of $p(\eta, \phi, \theta|\mb{z})$
in terms of $\theta_1,\dots,\theta_{n}$,
$\phi_1,\dots,\phi_{M-1}$, $\mu_1,\dots,\mu_M$, $\Sigma_1,\dots,\Sigma_M$,
hyperparameter $\Gamma$,
and user-provided covariance matrices $S_1, \dots, S_L$ 
and weights $w_1, \dots, w_L$,
is proportional to
$$\prod_{m=1}^M\Bigg(|\Sigma_m|^{-\frac{n_m}2}
e^{-\frac12\tr(\Sigma_m^{-1}C_m)}
\sum_{i=1}^{n} e^{-\frac12(z_i-\mu_m)^T\Gamma^{-1}(z_i-\mu_m)}
\times \notag\\
\left|\Sigma_m\right|^{-d-2.5}
e^{-\frac12\tr\left(S_l\Sigma_m^{-1}\right)}\Bigg)
\prod_{i=1}^{n} 
\mathbb{I}_{\{\eta_1,\dots,\eta_n\}}(\theta_i)
\prod_{m=1}^{M-1} \phi_m^{n_m}(1-\phi_m)^{n_m^+}$$
where 
$n_m=\sum_{i=1}^{n} \mathbb{I}_{\{\eta_m\}}(\theta_i)$,
$n_m^+={n}-\sum_{m'=1}^m n_{m'}$, and
$C_m=\sum_{i: \theta_i=\eta_m} (z_i-\mu_m)(z_i-\mu_m)^T$.

We derive the conditional distributions from (\ref{eqn:post2})
by analyzing the distribution of
each $\mu_m$, $\Sigma_m$, $\phi_m$, or $\theta_i$ with all other
variables held constant. A Gibbs sampler designed with these conditionals
provides a numerical estimate
of the posterior distribution for 
$p(\eta,\phi,\theta|\mb{z})$ of 
(\ref{eqn:post1}) \citep{gelfand1990sampling}.
After initialization of $\phi$ and $\eta$,
the Gibbs sampler operates by iterating the following.

* Update each $\theta_i$, one at a time,
as a random draw from the discrete distribution $\{\eta_m: m=1, \dots,M\}$
with probabilities proportional to $\pi_m\,N_{d+1}(z_i|\mu_m,\Sigma_m)$.
* Update each $\phi_m$ for $m=1,\dots,M-1$, one at a time,
from its conditional posterior distribution, Beta$(n_m+1, n_m^++1)$.
* Update each $\eta_m$, one at a time, by
  * first drawing $\mu_m$ from a multivariate normal distribution
according to its conditional parameters (shown below),
  * then drawing $\Sigma_m$ from an inverse-Wishart distribution
according to its conditional parameters (shown below).


## Conditional posterior distribution of each $\mu_m$
For any particular $\mu_m$,
its conditional density in (\ref{eqn:post2})
is proportional to
$$e^{-\frac12\sum_{i'=1}^{n}\mathbb{I}_{\eta_m}\!(\theta_{i'})
(z_{i'}-\mu_m)'\Sigma_m^{-1}(z_{i'}-\mu_m)}
\sum_{i=1}^{n} 
e^{-\frac12(z_{i}-\mu_m)'\Gamma^{-1}(z_{i}-\mu_m)}.$$

In the first exponential term %of (\ref{eqn:mu1}) 
only $z_i$ with $\theta_i$ currently assigned 
to component $m$ affect the distributional form,
reducing to
$e^{n_m \bar{z}_m'\Sigma_m^{-1}\mu_m-\frac12 n_m \mu_m'\Sigma_m^{-1}\mu_m}$,
where $n_m\bar{z}_m=\sum_{i'=1}^{n}\mathbb{I}_{\{\eta_m\}}\!(\theta_{i'})z_{i'}$.

Rearranging the exponential terms, the posterior conditional density
of $\mu_m$ is proportional to the $z_i$-mixture of 
$n$ multivariate Gaussian densities
$$\sum_{i=1}^{n} 
\left(e^{-\frac12\mu_m'\left(\Gamma^{-1}+n_m\Sigma_m^{-1}\right)\mu_m}\right)
\left(e^{-\frac12 z'_i\Gamma^{-1}z_i}\right)
\left(e^{\left(z_i'\Gamma^{-1}+n_m\bar{z}_m'\Sigma_m^{-1}\right)\mu_m}\right).$$
Each $z_i$ multivariate normal density in this mixture has
variance 
$\left(\Gamma^{-1}+n_m\Sigma_m^{-1}\right)^{-1}$
and mean equal to its variance times
$\left(\Gamma^{-1}z_i+n_m\Sigma_m^{-1}\bar{z}_m\right)$.

As the distribution for $\mu_m$ is a mixture of 
multivariate normal density components,
the Gibbs sampler will need to first select
a component from this mixture. 
The component selected should be equal to the 
marginal density of (\ref{eqn:mu4}) with respect to the $z_i$'s of the mixture.
This marginal density reduces to
$$e^{-\frac{n_m}2(z_i-\bar{z}_m)'
\Gamma^{-1}(\Gamma^{-1}+n_m\Sigma_m^{-1})^{-1}\Sigma_m^{-1}
(z_i-\bar{z}_m)},$$
attained by integrating out
the $\mu_m$ over each $z_i$ density. 
The component selection for $z_i$ is multinomial 
with probabilities proportional to those
shown in (\ref{eqn:mu6}) for $i=1,\dots, n$.

## Conditional posterior distribution of each $\Sigma_m$
The conditional posterior density for 
each $\Sigma_m$ in (\ref{eqn:post2}), 
independent of the other component variances,
is 
$$\sum_{l=1}^L w_l 
\left|S_l\right|^{\frac{d+3}2}
\left|\Sigma_m\right|^{-\frac{n_m+2d+5}2}
e^{-\frac12\tr\left((S_l+C_m)\Sigma_m^{-1}\right)}$$
that is a mixture 
composed of $L$ inverse-Wishart densities.
Each $S_l$ component density 
follows an inverse-Wishart distribution for $\Sigma_m$
with $n_m+d+3$ degrees of freedom
and scale matrix $(S_l + C_m)^{-1}$. 
\citep{gelman2013bayesian}. (using BDA2 pp.574-5)

The component density of the mixture selected in the Gibbs sampler
is equal to the $L$ marginal probabilities %of each $S_l$ component 
of (\ref{eqn:sigma2}).
The selection of component $S_l$ is multinomial 
with probability proportional to
$w_l \left|M_l\right|^{\frac{d+3}2} \left|M_l+S_m\right|^{-\frac{n_m+d+3}2}.$

